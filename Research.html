<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

<!-----------HEADLINE IN BROWSER------>

  <head>
<title>Boris Bolliet</title>
<meta http-equiv="content-type" content="text/html;charset=iso-8859-2" />



<link rel="stylesheet" href="style.css" type="text/css" />
</head>




<body>
<div class="content">
 
  <div class="header">
    <div class="title">Boris Bolliet</div>
    <div class="slogan">PhD Student, <i>"The expanding universe: primordial and contemporary acceleration."</i><br/>
      <h3>Laboratoire de Physique Subatomique et de Cosmologie (LPSC Grenoble, France) <br>&Eacute;cole Normale Sup&eacute;rieure de Lyon (ENS Lyon, France)<h3/>
   </div>
      </div>

 

<!-----------NAVIGATOR------>

      
  <div id="nav">
    <ul>
      <li><a href="index.html">Home</a></li>
      <li><a href="About.html">About</a></li>
      <li id="current"><a href="Research.html">Research</a>
          <ul>
	<li><a href="Research.html#ResearchInterest">Research interests</a></li>
	<li><a href="Research.html#ResearchTools">Research tools</a></li>
	  </ul>
	  </li>
      <li><a href="Teaching.html">Teaching</a>
      <ul>
	<li><a href="Teaching.html#NumericalComputing">Numerical computing</a></li>
	<li><a href="Teaching.html#DataAnalysis">Data analysis</a></li>
        <li><a href="Teaching.html#NuclearPhysics">Nuclear physics</a></li>
      </ul>
      </li>
      <li><a href="ShortReviews.html">Short reviews</a>
      <ul>
	<li><a href="ShortReviews.html#Online">Websites</a></li>
	<li><a href="ShortReviews.html#Books">Books</a></li>
	<li><a href="ShortReviews.html#PhysicsArticles">Articles</a></li>
      </ul>
      </li>
   <li><a href="Contact.html">Contact</a></li>
    </ul>
  </div>

  
  <hr>
<!-----------NAVIGATOR------>


      
    <div class="main_content">

 
      <div class="text_padding">
        <h2 id="ResearchInterests">Research interests</h2>
	<h1>I am interested in understanding the causes of the accelerated expansion of the universe, which seems to happen at both ends of the cosmic history.<br/><br/>

	  With my research group in Grenoble we are working on the CMB phenomenology of Loop Quantum Cosmology. We constrain LQC models by confronting them to CMB measurement.<br/><br/>

          With Richard Battye in Manchester, I am studying the phenomenology of modified theories of gravity using the Equation of State approach to linear perturbations.
<br/><br/>Finally, with Juan Macias Perez, Barbara Commis and Eiichiro Komatsu, I am investigating the constraints that can be set on different cosmological parameters based on the Planck 2015 data for the thermal Sunyaev Zel'dovich power spectrum.



        </h1> 	  
	<h4>Primordial inflation</h4>
	
	   <h1>Measurement of the Cosmic Microwave Background (CMB) has raised conceptual problems for Cosmology. How can the observable universe be so homogeneous at very large scales? Why does it appear so spatially flat? These issues are solved very elegantly by assuming a phase of accelerating expansion (inflation) before the production of the CMB.<br/><br/>
	     The inflationary paradigm is expected to be beyond reach of the forthcoming CMB experiments. A measurement of the primordial contribution to the angular power spectrum of the CMB B-modes would be a direct evidence for inflation. Moreover, one could start adressing experimentally even more fundamental questions:<br/><br/>
-What is the universe made of during inflation? Is it a scalar field, similar to the ones predicted by the Standard Model? Or, is inflation a consequence of modifications to standard General Relativity? <br/><br/>
	     -How does inflation ends? At the end of inflation, can we describe a physical process able to produce the particles of the Standard Model?<br/><br/>
	     -How does inflation begins? Starting with a universe at the Planck density, how generic is inflation?<br/><br/></h1>
	     
	   <h4>Dark energy</h4>
	   
	      <h1> The present accelarating expansion of the universe is a recent experimental discovery (see <a href="http://www.nobelprize.org/nobel_prizes/physics/laureates/2011/advanced-physicsprize2011.pdf" target="_blank">here</a>). The simplest theoretical model to predict the accelerating expansion is the &Lambda;CDM cosmological scenario. Within this model, the acceleration of the universe is due to a non-zero cosmological constant. Dark energy would then be a homogenous fluid filling the universe, with constant energy density and an equation of state strictly equal to minus one. <br/><br/>
	       It is very tempting to interprete dak energy as vacuum energy, a well known (and observed) prediction of relativistic quantum mechanics.<br/><br/>

	       Integrated over the volume of the observable universe, the vaccuum energy is expected to be larger than the energy corresponding to the cosmological constant by a hundred and twenty two orders of magnitude. There are three ways to cop with the situation.<br/><br/>

	       1) One could say that, for a volume such as the volume of the universe, the calculation of the vacuum energy fails due to our misunderstanding of quantum mechanics on curved space-time, but: dark energy <i>is</i> vacuum energy. The interpretation of <i>vacuum</i> should be reconsidered in order to predict the right value.<br/><br/>

	       2) One could not be worried of giving a physical interpretation to dark energy: the cosmological constant should be treated on the same footing as the other fundamental constants of physics, avoiding the conflict between dak energy and vaccuum energy. (But then, the interpretation of <i>vacuum</i> should still be reconsidered, or a new phenomenon should be included to counterbalance vacuum energy, making it negligible on cosmological scales.) <br/><br/>

	       3) The last option is to reject the &Lambda;CDM model and General Relativity on the largest cosmological scales. In fact, there are many modified gravity theories in agreement with observations. Here also, the puzzle of vacuum energy is not adressed but rather sent to other domains of physics.</h1> 
		
		<h4>Loop Quantum Gravity and Quantum Cosmology</h4>
		   <h1> I am of the opinion that Loop Quantum Gravity (LQG) is potentially able to predict inflation and lead to the right interpretation of <i>vacuum</i>.<br/><br/>
		    In LQG, space-time itself is considered as a quantum field, whose canonical quantum variables are the holonomy of the Ashtekar connection, and the flux of the densitized tetrad field.<br/><br/>

		    The quantum states of space-time are called spin network states, and their dynamics is called spinfoam.<br/><br/>

		    The cosmological models deduced from LQG (Loop Quantum Cosmology and Spinfoam Cosmology) are free of space-time singularities: the big bang and black holes are replaced by a bounce and Planck stars, respectively. </h1>



	  	  <br/>
	  <br/>
	  <h2 id="ResearchTools">Research tools</h2>

 
      
  <h4 id="CLASSandMontePython">Cosmic Anisotropy Solver System</h4>
    <h1>The <a href="http://class-code.net/" target="_blank">Cosmic Anisotropy Solver System</a> (CLASS) is a code written by Pr. <a href="https://lesgourg.web.cern.ch/lesgourg/presentation.html" target="_blank">Julien Lesgourgues</a> and Dr. <a href="http://www.icg.port.ac.uk/author/tramt/" target="_blank">Thomas Tram</a>.
      		      <br/>
		      <br/>
		      CLASS is a Boltzmann code, analogous to CAMB but however written in C++. It is aimed at computing the predicted anisotropy power spectra of the CMB from a given &Lambda;CDM scenario (i.e. a specific setting of the free-parameters of the  &Lambda;CDM model). The first public version has been released in 2011.
		      <br/>
		      <br/>
		      Some useful notations used throughout the code are: <br/> <br/>

		      <hc>
ppr</hc> <ml><ml> pointer to precision structure <br/>
 <hc>pba</hc> <ml><ml> pointer to background structure <br/>
 <hc>ppt</hc> <ml><ml> pointer to the perturbation structure <br/>
 <hc>pps</hc> <ml><ml> pointer to the spectra structure <br/>
 <hc>index_md </hc><ml>index of mode under consideration (scalar/.../tensor) <br/>
 <hc>k</hc><ml><ml><ml>wavenumber <br/>
 <hc>tau</hc>  <ml><ml> conformal time <br/>
 <hc>y</hc><ml><ml><ml>vector of perturbations (those integrated over time)  <br/>
 <hc>ppw</hc> <ml><ml> contains the updated metric perturbations
     <br/>
     <br />
The physical constants are:<br>
<br />
 _Mpc_over_m_ 3.085677581282e22 >>conversion factor from meters to megaparsecs <br>
remark: CAMB uses 3.085678e22: good to know if you want to compare  with high accuracy<br>
<br />
_Gyr_over_Mpc_ 3.06601394e2 >>conversion factor from megaparsecs to gigayears
				         (c=1 units, Julian years of 365.25 days) <br><br />
_c_ 2.99792458e8            >> c in m/s<br>
_G_ 6.67428e-11             >> Newton constant in m^3/Kg/s^2<br>
_eV_ 1.602176487e-19        >> 1 eV expressed in J<br>
<br />
Parameters entering in Stefan-Boltzmann constant sigma_B<br>
_k_B_ 1.3806504e-23<br>
_h_P_ 6.62606896e-34<br>
      <br />
      The number pi<br>
_PI_ 3.1415926535897932384626433832795e0 >> <br />
                                                      <br>
                                                        
All quantities in the code are either dimensionless or written in unit Mpc^n. In the "ini" file, some parameters either start with an upper case or a lower case caracter. The lower case parameter equals the upper case parameter times h^2=0.49.

     
</h1>
<br>
   <hst>Main</hst>
  <br><br> <h1>The <hc>int main</hc> of CLASS, that is written in the file <hc>class.c</hc>, consists in a succession of check that the following functions evaluate to <hc>SUCCESS</hc>:<br><br>
	 <div class="code">
	   input_init_from_arguments(argc,argv,&pr,&ba,&th,&pt,&tr,&pm,&sp,&nl,&le,&op,errmsg)<br>
	     background_init(&pr,&ba)<br>
	     thermodynamics_init(&pr,&ba,&th)<br>
	     perturb_init(&pr,&ba,&th,&pt)<br>
	     primordial_init(&pr,&pt,&pm)<br>
	     nonlinear_init(&pr,&ba,&th,&pt,&pm,&nl)<br>
	     transfer_init(&pr,&ba,&th,&pt,&nl,&tr)<br>
	     spectra_init(&pr,&ba,&pt,&pm,&nl,&tr,&sp)<br>
	     lensing_init(&pr,&pt,&sp,&nl,&le)<br>
	     output_init(&ba,&th,&pt,&pm,&tr,&sp,&nl,&le,&op)<br>
	   </div><br>
	     These functions take the <i>addresses</i> of the following structures as arguments.<br><br>
	     <div class="code">
	       struct precision pr;      <br>
  struct background ba;       <br>
  struct thermo th;            <br>
  struct perturbs pt;          <br>
  struct transfers tr;         <br>
  struct primordial pm;         <br>
  struct spectra sp;           <br>
  struct nonlinear nl;          <br>
  struct lensing le;           <br>
  struct output op;             <br>
  ErrorMsg errmsg;             <br>
  </div>
  <br>
				     The header <hc>class.h</hc> contains the declaration of the standard libraries as well as the declaration of the ten class modules, that correspond to the structures mentioned above (except for <hc>precision</hc> which is a <i>global</i> structure defined in <hc>common.h</hc>), <i>i.e.</i><br><br>
				       <div class="code">
#include "input.h"<br>
#include "background.h"<br>
#include "thermodynamics.h"<br>
#include "perturbations.h"<br>
#include "primordial.h"<br>
#include "nonlinear.h"<br>
#include "transfer.h"<br>
#include "spectra.h"<br>
#include "lensing.h"<br>
#include "output.h"<br>
</div>
<br>and the declaration of a file  <hc>common.h</hc> and the <i>tools</i> header files (<i>e.g.</i> for the Runge-Kutta method).<br>
  </h1>
<br>
  <hst>Common parameters and functions</hst>
  <br><br>
<h1>Parameters and functions that are used by all modules are declared and defined in <hc>common.h</hc>. This includes the definition of mathematical constants such as Pi, macros for the max and min of two numbers, <i>error reporting and testing macros</i> such as <hc>class_call</hc> and <hc>class_test</hc>, <i>allocating memory macros</i> such as <hc>class_alloc(pointer,size,errormessage)</hc> and similar commodities.<br><br>
	
	A more interesting part is the declaration of parameters related to the precision of the code and to the method of calculation. <br><br>
	    <div class="code">
	      enum evolver_type {rk,ndf15};<br>
		enum pk_def {delta_m_squared,delta_tot_squared,<br>
		  <ml><ml><ml><ml><ml>delta_bc_squared,delta_tot_from_poisson_squared};<br>
			      enum file_format {class_format,camb_format};
			    </div><br>
			      The <i>precision parameters</i> refer to the members of the structure <hc>precision</hc> (only a few illustrative members are listed bellow):<br><br>
			      <div class="code">
				<b>struct precision</b>{<br>
				  double a_ini_over_a_today_default;<br>
				      double back_integration_stepsize;<br>
  double tol_background_integration;<br>
  double safe_phi_scf;<br>
  FileName sBBN_file;<br>
  double recfast_z_initial; <br>
double reionization_z_start_max;<br>
enum evolver_type evolver;<br>
 double k_min_tau0;<br>
 double k_bao_center;<br>
  int tight_coupling_approximation;<br>
int ur_fluid_approximation;<br>
 double k_per_decade_primordial;<br>
 double halofit_dz; <br>
 int accurate_lensing;};<br>
</div>

			      
</h1>
  
   <br>
       <hst>Input parameters</hst><br><br>
    <h1> In the file <hc>input.h</hc> there are in particular the definitions of the macros necessary to reads input parameters from the <hc>ini</hc> file, <i>e.g.</i><br><br>
       <div class="code">
	 class_read_double(name,destination)<br>
	   class_read_int(name,destination)<br>
	     class_read_string(name,destination)<br>
	     </div>
	     <br>
	       These macros are used in the module <hc>input.c</hc> whose aim is to <i>read</i> all the input parameters. An important function defined in <hc>input.c</hc>, schematically:<br><br>
	       <div class="code">
<b>int input_read_parameters</b>(<br>
                          struct file_content * pfc,<br>
                          struct precision * ppr,<br>
                          struct background *pba,<br>
                          struct thermo *pth,<br>
                          struct perturbs *ppt,<br>
                          struct transfers *ptr,<br>
                          struct primordial *ppm,<br>
                          struct spectra *psp,<br>
                          struct nonlinear * pnl,<br>
                          struct lensing *ple,<br>
                          struct output *pop,<br>
                          ErrorMsg errmsg<br>
                          ) {<br><br>
class_read_double("a_today",pba->a_today);<br><br>
 if (pba->Omega0_fld != 0.) {<br>
    class_read_double("w0_fld",pba->w0_fld);<br>
    class_read_double("wa_fld",pba->wa_fld);<br>
class_read_double("cs2_fld",pba->cs2_fld);}<br><br>
 if (strcmp(string1,"analytic_Pk") == 0) {<br>
      ppm->primordial_spec_type = analytic_Pk;<br>
      flag2=_TRUE_;}<br><br>
....}
						       
						       <br>
	</div>
						       <br>
Two other functions should be mentionned:<br><br>

<hc><b>int input_default_params</b></hc> where the default values of the cosmological parameters are given.<br><br>
<hc><b>int input_default_precision</b></hc> which contains <i>e.g.</i> <hc>ppr->a_ini_over_a_today_default = 1.e-14;</hc>.

</h1>
<br>


<hst>Adding a new parameter to CLASS</hst><br><br>
    <h1>There are several changes to done in order to add a new input parameter to CLASS. In <hc>input.c</hc>:<br>
<ul>
<li> Add it to the "ini" file with the desired value. <br>
<li> Declare the new parameter externally, <i>e.g.</i> in a header file.<br>
<li> Assign a default value to the parameter in <hc>int input_default_params()</hc><br>
<li> In <hc>int input_read_parameters()</hc>, read the parameter with <i>e.g.</i> <hc>class_read_int("newparam",newparam)</hc> 
</ul>
      
<br><br>

    <div class="code">
      


</div>
</h1>
  
<hst>Background cosmology</hst><br><br>

    <h1>The background module enables to compute the background dynamics with given input parameters.The background sctructure is defined in <hc>background.h</hc>. Some illustrative members are reported bellow.
<br><br>
<div class="code">
<b>struct background</b>{<br>
double H0;<br>
double w0_fld; <br>
double phi_prime_ini_scf;<br>
double h;<br>
double conformal_age;<br><br>
int index_bg_a; <br>
int index_bg_H; <br>
int index_bg_rho_cdm;<br> 
int index_bg_rho_crit; <br>
int index_bg_conf_distance;<br>
int index_bg_ang_distance;<br>
int index_bg_D;<br><br>
double * tau_table;<br>
double * z_table;<br>
double * background_table;<br><br>
int index_bi_a;<br>  
int index_bi_rho_dcdm;<br><br>
short has_cdm;<br>
short has_lambda;}<br>
</div>
<br>
where <hc>D</hc> refers to the <i>growth factor in dust universe</i>. Moreover, <hc>bt</hc> stands for background table, <hc>bi</hc> for background integration.
    
<br><br>
Some indications found in the preamble of <hc>background.c</hc>:<br><br>
<div class="code">
 - background_functions() returns all background quantitites {A} as a function of quantitites {B}.<br><br>
 - background_solve() integrates the quantities {B} and {C} with respect to conformal time.<br>
</div>
<br>
where {A}, <i>e.g.</i> rho_gamma, can be expressed as simple analytical functions of a few variables {B}, <i>e.g.</i> scale factor. And  some other quantitites, called {C}, <i> e.g.</i> the sound horizon or proper time, also require an integration with respect to time, that cannot be infered analytically from parameters {B}.<br><br>

Some functions that are declared in <hc>background.h</hc> and defined in <hc>background.h</hc>:<br><br><div class="code">
int background_at_tau()<br>
int background_functions()<br>
int background_tau_of_z()<br>
int background_init()<br>
int background_solve()<br>
int background_initial_conditions()<br>
int background_output_data()<br>
</div>

</h1><br>

<div class="code">  

<br><br>
                      
/*********************************************
//Example: Print background data vs Redshift

   for (index_tau=0; index_tau<pba->bt_size; index_tau++)
    {

    printf(
	    "tau=%e z=%e a=%e H=%e\n",
	    pba->tau_table[index_tau],
	    pba->z_table[index_tau],
	    pba->background_table[index_tau*pba->bg_size+pba->index_bg_a],
	    pba->background_table[index_tau*pba->bg_size+pba->index_bg_H]);

    }
**************************************/
/***********************************************************/
</div>
<br><br>
<hst>Anisotropy and Fourier power spectra</hst><br><br>

    <h1>The spectra module is dedicated to the computation of anisotropy and Fourier power spectra. The spectra structure, <hc>struct spectra</hc> is defined in <hc>spectra.h</hc>. Once initialized by <hc>spectra_init()</hc>, it contains a table of all C_l's and P(k) as a function of multipole/wavenumber, mode (scalar/tensor), type (for C_l's: TT, TE, etc.), and pairs of initial conditions (adiabatic, isocurvatures).<br><br>
    
Important functions include:<br><br>

<div class="code">
int spectra_cl_at_l()<br>
int spectra_pk_at_z()<br>
int spectra_pk_at_k_and_z()<br>
int spectra_cls()<br>
int spectra_compute_cl()<br>
int spectra_k_and_tau()<br>
int spectra_pk()<br>
int spectra_sigma()<br>
int spectra_matter_transfers()<br>
</div>
<br>

Preamble of <hc>spectra.c</hc>:
<br><br>

<i>This module computes the anisotropy and Fourier power spectra C_l, P(k)'s given the transfer and Bessel functions (for anisotropy spectra), the source functions (for Fourier spectra) and the primordial spectra. The following functions can be called from other modules:</i>
 <ul>
 <li><hc>spectra_init()</hc> at the beginning (but after <hc>transfer_init()</hc>),</li>
 <li><hc>spectra_cl_at_l()</hc> at any time for computing C at any l,</li>
 <li><hc>spectra_spectrum_at_z()</hc> at any time for computing P(k) at any z,</li>
 <li><hc>spectra_spectrum_at_k_and z()</hc> at any time for computing P at any k and z,</li>
 <li><hc>spectra_free()</hc> at the end.</li>
   </ul>


</h1>
<br>

<hst>Transfer functions</hst>
<br><br>
<h1>
(Preamble of <hc>transfer.c</hc>.) This module has two purposes:
 <ol>
  <li>At the beginning, to compute the transfer functions in harmonic space, Delta_l(q), and store them in tables used for interpolation in other modules.</li><br>
  <li>At any time in the code, to evaluate the transfer functions (for a given mode, initial condition, type and multipole l) at any wavenumber q (by interpolating within the interpolation table).</li>
</ol>
Hence the following functions can be called from other modules:
 <ul>
 <li><hc>transfer_init()</hc> at the beginning (but after <hc>perturb_init()</hc> and <hc>bessel_init()</hc>),</li>
 <li><hc>transfer_functions_at_q()</hc> at any later time,</li>
 <li><hc>transfer_free()</hc> at the end.</li>
</ul>
The structure <hc>transfers</hc> is defined in <hc>transfer.h</hc>.
</h1>
<br>

<hst>Cosmological perturbations</hst>
<br><br>
<h1>
(Preamble of <hc>perturbation.c</hc>.) This module has two purposes:
 <ol>
  <li>At the beginning, to initialize the perturbations, <i>i.e.</i> to integrate the perturbation equations, and store temporarily the terms contributing to the source functions as a function of conformal time. Then, to perform a few manipulations of these terms in order to infer the actual source functions S(k,tau), and to store them as a function of conformal time inside an interpolation table.</li><br>
  <li>At any time in the code, to evaluate the transfer functions (for a given mode, initial condition, type and multipole l) at any wavenumber q (by interpolating within the interpolation table).</li>
</ol>
Hence the following functions can be called from other modules:
 <ul>
   <li><hc>perturb_init()</hc> at the beginning (but after <hc>background_init()</hc> and <hc>thermodynamics_init()</hc>),</li>
 <li><hc>perturb_sources_at_tau()</hc> at any later time,</li>
 <li><hc>perturb_free()</hc> at the end.</li>
</ul>
The structure <hc>perturbs</hc> is defined in <hc>perturbation.h</hc>. Note:<br><br>

Flags for various approximation schemes<br>
<ul>
  <li><hc>tca</hc> = tight-coupling approximation,</li>
 <li><hc>rsa</hc> = radiation streaming approximation,</li>
 <li><hc>ufa</hc> = massless neutrinos / ultra-relativistic relics fluid approximation).</li>
</ul>
Integration is made either in the synchronous or conformal Newtonian gauge.
</h1>


<br><br>
<hst>Class Plotting Utility</hst>
<br><br>
<h1>

Class Plotting Utility is  written in python. It enables you to quickly produce
nice plots for some given data files.<br><br>

The data file should be either a 'txt' or a 'dat' file. It should include a header with lines starting with '#'. 
<br><br>
<div class="code">
$ python CPU.py ../DataFiles/FinalData/LQC.txt ../DataFiles/Pk_ref_CLASS/StandardInflation.dat -y P --scale loglog<br>
$ python CPU.py output/Pk_LQCcl_lensed.dat output/Pk_LQCcl.dat  output/Pk_refcl_lensed.dat output/Pk_refcl.dat -y TT BB --scale loglog
</div>
<br>
</h1>
		  <h4>Montepython and the Planck Likelihood</h4>
		  <h1>The <a href="http://montepython.net/" target="_blank">Monte Python</a> code, is a Monte Carlo code written in Python to be used with CLASS. It has been written by Dr. <a href="http://baudren.github.io/index.html" target="_blanck">Benjamin Audren</a> and is being used by the Planck Collaboration for parameters extraction.

		      <br/>
                      <br/> 
		     Monte Python is the software that finds the best-fit to the CMB data, according to the specific cosmological scenario (&Lambda;CDM, Modified Gravity, Massive Neutrinos, etc.) that is set into CLASS (just as is <a href="http://cosmologist.info/cosmomc/" target="_blank">CosmmoMC</a> with <a href="http://camb.info/" target="_blank">CAMB)</a>.
		      <br/>
		      <br/>
		      Running Monte Python implies that the data and corresponding likelihoods are available to the Monte Carlo code. The Planck Data and Likelihood (2015 release) are publicly available one the <a href="http://pla.esac.esa.int/pla/#cosmology" target="_blank">Plank Legacy Archive</a> website.
		      <br/>
		      <br/>
		      The two most important files are <hc>COM_Likelihood_Code-v2.0_R2.00.tar.gz</hc>, the code that <i>installs</i> the Planck likelihood, and <hc>COM_Likelihood_Data-baseline_R2.00.tar.gz</hc>, the data.
		      <br/>
		      <br/>
		      There is a detailed description of the contents of these folders on the ESA wiki page <a href="http://wiki.cosmos.esa.int/planckpla2015/index.php/CMB_spectrum_%26_Likelihood_Code" target="_blank">CMB spectra and likelihood code</a>. 
		      <br/>
		      <br/>
		      The Planck Collaboration uses the following labels for likelihoods (cf. <a href="http://arxiv.org/abs/1502.01589" target="_blank">Planck 2015 results. XIII. Cosmological parameters</a>, footnote 7, p. 6.):
   <br/>
		      <br/>
		      (i) <b>Planck TT</b>: combination of the TT likelihood at multipoles l >= 30 and a low-l temperature-only likelihood based on the CMB map recovered with Commander;
		         <br/>
		      <br/>
		      (ii) <b>Planck TT+lowP</b>: further includes the Planck polarization data in the low-l likelihood;
		         <br/>
			 <br/>
			 (iii) <b>Planck TE+lowP</b>: TE likelihood at l >= 30 plus the polarization-only component of the map-based low-l Planck likelihood;   <br/>
		      <br/>
		      (iv) <b>Planck TT,TE,EE+lowP</b>: combination of the likelihood at l >= 30 using TT, TE, and EE spectra and the low-l temperature+polarization likelihood.
   <br/>
		      <br/>
		     (v) <b>Planck TE+lowT,P</b>: combinations of the polarization likelihoods at l >= 30 and the temperature+polarization data at low-l.
		      <br/>
		      <br/>
</h1>
		      <hst>Installing and running CLASS-Montepython with the Planck data</hst>
		      <br/>
		      <br/>
<h1>		      Once the four <hc>gz</hc> files are in the same repository, say ClassAndMontepython/, open a terminal and unzip them one by one using
		      <br/>
		      <br/>
		      <hc>tar xzvf class_public-2.4.2.tar.gz<br/>
			tar xzvf montepython_public-2.1.4.zip<br/>
			tar xzvf COM_Likelihood_Data-extra-plik-DS_R2.00.tar.gz<br/>
			tar xjvf COM_Likelihood_Code-v2.0_R2.00.tar.bz2</hc>
			<br/>
			<br/>
			(Pay attention to the <hc>xjvf</hc> and not <hc>xzvf</hc> for the bz2 file.) Move all the zip files to a ZIPfiles/ folder, or delete them in order to make your repository tidy.
			<br/>
			<br/>
			Now there should be four new folders inside ClassAndMontepython/, they are: 1) class_public-2.4.2/ containing CLASS Code; 2) montepython_public-2.1.4/ for Monte Python; 3) plc-2.0/ for the Planck likelihood; 4) plc_2.0/ for the Planck data.
			<br/>
			<br/>
			<b>Installing the Planck likelihood</b>
			<br/>
			<br/>
			In order to install the Planck likelihood, move to plc-2.0/. If you have a recent and tidy computer this step may take only a few seconds, using a tool called <hc>waf</hc> provided inside the plc-2.0/ folder. On Mac, just type:
		      <br/>
		      <br/>
		      <hc>./waf configure --install_all_deps <br/>
			./waf install</hc>
			<br/>
			<br/>
Although, before that you might want to do:
		      <br/>
		      <br/>
		      <hc>sudo port selfupdate <br/>
			sudo port upgrade outdated</hc>
			<br/>
			<br/>


          
			On Linux (with <hc>ifort</hc> and <hc>mkl</hc>):
		      <br/>
		      <br/>
		      <hc>./waf configure --install_all_deps --lapack_mkl=$MKLROOT<br/>
			./waf install</hc>
		      			<br/>
					<br/>
					On the cluster, at CC-in2p3, I had to do:
			<br/>
			<br/>
			<hc>./waf configure --install_all_deps --lapack_mkl=/usr/local/intel/mkl/ --ifort<br/>
													    ./waf install</hc>
			<br/>
			<br/>
		If this procedure fails, you should install using <hc>make</hc>, after having checked that all the necessary libraries are well linked (see <hc>readme.md</hc> inside plc-2.0/).
			<br/>
			<br/>
                        Then you can copy the line:<br>
                          <br>
                     <hc>source /Users/borisbolliet/Dropbox/SZ/Codes/plc-2.0/bin/clik_profile.sh</hc>
                     <br>
                       <br>
                         into your bash_profile and do:
                         <br>
                           <br>
                             <hc>
                               . ~/.bash_profile
                             </hc>
                             <br>
                               <br>
                                 to reload your profile file. 
			<br/>
			<br/>
			The installation guides for CLASS and its Python wrapper can be found <a href="https://github.com/lesgourg/class_public/wiki/Installation" target="_blank">here</a> and <a href="https://github.com/lesgourg/class_public/wiki/Python-wrapper" target="_blank">there</a>, respectively. The installation guide for Monte Python is <a href="http://montepython.net/" target="_blank">here</a>, under "Installation" and/or "Documentation".
			<br/>
			<br/>
In the following, I give a description of all the necessary steps for a basic installation of these softwares, in their simplest configuration. 
			<br/>
			<br/>
			<b>Installing CLASS and its Python wrapper</b>
			<br/>
			<br/>
			Installing CLASS is generally straightforward. Move to  class_public-2.4.2/, and type the following commands:
		      <br/>
		      <br/>
		      <hc>make clean<br/>
			make -j</hc>
			<br/>
			<br/>
			The second line ensure that the Python wrapper is installed along with CLASS. You should have the SciPy stack installed, by doing:
			<br/>
			<br/>
			<hc>$ sudo apt-get install python-numpy python-scipy python-matplotlib ipython ipython-notebook python-pandas python-sympy python-nose</hc>
			<br/>
			<br/>
			Check that the installion of CLASS is successfull by runing:
		      <br/>
		      <br/>
		      <hc>./class explanatory.ini</hc>
		      <br/>
			<br/>
			In order to check that the Python wrapper is well installed, run <hc>python</hc> in the terminal, and then:
		      <br/>
		      <br/>
		      <hc>>>from classy import Class</hc>
			<br/>
			<br/>
If there is no error message, your installation of the Python wrapper of CLASS is probably successfull. The ultimate check is to run a test Python scripts that calls CLASS. Move into the subfolder /python, inside class_public-2.4.2/, and run the following command in your terminal (after having exited python):
		      <br/>
		      <br/>
		      <hc>nosetests test_class.py</hc>
			<br/>
			<br/>
			Note that you need to have the <hc>nose</hc> module installed (<hc>pip install nose</hc>) along with its submodule <hc>nose-parameterized</hc>. If <hc>nose-parameterized</hc> is not present on your computer, even when <hc>nose</hc> is installed, download the wheel file (you might have to do <hc>pip install wheel</hc>) <hc>nose_parameterized-0.5.0-py2.py3-none-any.whl</hc> at the bottom of <a href="https://pypi.python.org/pypi/nose-parameterized/" target="_blank">this</a> page, put it in class_public-2.4.2/python/, where you are, and run:
			<br/>
		      <br/>
		      <hc>pip install nose_parameterized-0.5.0-py2.py3-none-any.whl</hc>
			<br/>
			<br/>
			Then, try again  <hc>nosetests test_class.py</hc>, it will loop over many different setting of the cosmological parameters and return the common CLASS output in the terminal. For instance, while looping, when the output says<br/>
			<br/>
<hc>	
| Test case lensing=yes_output=tClpCllClP_k_initype=inflation_V_modes=st |	<br/>
</hc>
			<br/>
it means that: lensing is taken into account for the computation of the angular power spectra; the code is asked to compute the temperature "tCl", polarization "pCl" and lensed "lCl"; the initial primordial power spectrum is the one predicted by inflation; both scalar and tensor modes have to be considered. 
			<br/>
			<br/>
			(The same procedure shall be used again, when implementing modifications to the original CLASS code, in order to check that the code still runs perfectly, note that the loop takes about three hours to complete) 
			<br/>
			<br/>
			<b>Installing Monte Python</b>
			<br/>
			<br/>
			In fact, properly speaking Monte Python doesn't require an <i>installation</i>, it needs to be well linked to CLASS, the Planck likelihood and the Planck data. The key file for specifying the <i>paths</i> is <hc>default.conf.template</hc> inside the folder montepython_public-2.1.4/.
			<br/>
			<br/>
			First of all, create a copy of <hc>default.conf.template</hc> and name it <hc>default.conf</hc>. Open <hc>default.conf</hc> with a text editor. Three paths have to be specified <hc>default.conf</hc>.
			<br/>
			<br/>
			The path to the codes, "<hc>root</hc>":
			<br/>
			<br/>
			<hc>root = '/Users/borisbolliet/Desktop/ClassAndMontepython/'</hc>
			<br/>
			<br/>
			The path to CLASS, "<hc>cosmo</hc>":
			<br/>
			<br/>
			<hc>path['cosmo'] = root+'/class_public-2.4.2'</hc>
			<br/>
			<br/>
			The path to the Planck likelihood, "<hc>clik</hc>":
			<br/>
			<br/>
			<hc>path['clik'] = root+'/plc-2.0'</hc>
			<br/>
			<br/>
			The settings for the MCMC are specified in <hc>base2015.param</hc>, inside the folder montepython_public-2.1.4/.
			<br/>
			<br/>
			<b>Runing Monte Python</b>
			<br/>
			<br/>
			Go to  montepython_public-2.1.4/ and type the following commands
						<br/>
			<br/>
			<hc>$ source /Users/borisbolliet/Desktop/CLassAndMontepython/plc-2.0/bin/clik_profile.sh<br/>
			  $ montepython/MontePython.py run --conf default.conf -p base2015.param -o PlanckBaselineTest -c covmat/base.covmat -N 10
			</hc>
			<br/>
			<br/>
			The first command ensures that the paths to the likelihood are well set.You should include it into your .bash_profile (or .bashrc, or  .cshrc) file, otherwise each time you open a terminal you would need to type it againg in order to use Monte Python with the Planck likelihood. 
			<br/>
			<br/>
The file .bash_profile is generally in your home directory, you will find it with the command <hc>ls -al</hc>, and open it with <hc>open -a emacs .bash_profile</hc> (or <hc>emacs .bashrc &</hc> on Linux).
			<br/>
			<br/>
			The MCMC starts with the second command. The option <hc>-N 10</hc> means that the chain will be made of ten steps. A reasonnable number of steps for the chain to converge is 10000, and several dozens of chains are necessary to proceed to the parameter extraction in such an analysis. This is not doable on a single computer, motivating the use of a remote computing grid.
				<br/>
			<br/>
			The MCMC chains will be produced as a <hc>.txt</hc> file inside a newly created folder whose name is passed through the <hc>-o</hc> option (here, PlanckBaselineTest/). It is handy to create a directory chains/ inside montepython_public-2.1.4/ where all the future folders containing the chains will be stored.
			<br/>
			<br/>
			A nice exercise, doable on a laptop, is the <a href="http://supernovae.in2p3.fr/sdss_snls_jla/ReadMe.html" target="_blank">JLA<a> analysis. In montepython_public-2.1.4/data/JLA/ there is a <hc>readme.txt</hc> file that tells you how to download the data files manually. Basically, extract the arxiv http://supernovae.in2p3.fr/sdss_snls_jla/jla_likelihood_v4.tgz and copy all the files from jla_likelihood_v4/data/ to montepython_public-2.1.4/data/JLA/.
			    			<br/>
						<br/>

			    

			  You might need to install a few missing modules (<hc>pip install numexpr --user, pip install pandas --user	</hc>). Then, simply run						<br/>
			<br/>
			<hc>$ montepython/MontePython.py  run -p jla.param -o chains/JLAchains -N 1000</hc>
			<br/>
			<br/>
			Usually, the following warning appears:
			<br/>
			<br/>
					<div class="code">
		  
		  The acceptance rate is above 0.6, which means you might have difficulties
     exploring the entire parameter space. Try analysing these chains, and use
     the output covariance matrix to decrease the acceptance rate to a value
			  between 0.2 and 0.4 (roughly).</div>
			<br/>
			<br/>
			In order to analyze the chains do
			<br/>
			<br/>
			<hc>$ montepython/MontePython.py info chains/JLAChains
</hc>
			<br/>
			<br/>
			This commands produces a covariance matrix as well as a bestfit that can be used as inputs for the next run, as
			<br/>
			<br/>
			<hc>$ montepython/MontePython.py  run -p jla.param -o chains/JLAchains -c chains/JLAchains/JLAchains.covmat -b chains/JLAchains/JLAchains.bestfit  -N 1000</hc>
			<br/>
			<br/>
			Analyze the chains again and redo the procedure until the Gelman-Rubin convergence numbers (R-1) are small enough (tipycally <0.05) for all parameters.
			<br/>			<br/>
			<br/>
			<b>Plotting with montepython</b>
			<br/>
			<br/>
                        The plots are produce as an output of the command <hc>$ montepython/MontePython.py info ...</hc>. In particular, This command creates a subdirectory '/plots' in the chain directory, which contains the following two pdf files:<br>
                          
 <ul>
 <li>CHAINS_1d.pdf : the posterior probability for each parameter,</li>
 <li>CHAINS_triangle.pdf : the 2D posterior for all pairs of parameters.</li>
 </ul>
 <b>Make sure you run the command from outside the chain directory! And make sure the chain's names end with '__i.txt', with two underscores!</b>
On remote machines it is sometimes useful to disable the production of the pdf files, as follows:
			  <br/>
			  <br/>
			  <hc>$ montepython/MontePython.py info CHAINS --noplot</hc>
			  <br/>
			  <br/>
Then, only the covmat and bestfit will be computed. <br><br>

The dashed line that appears on the posterior plots corresponds to the mean likelihood, in order to remove it do
  			  <br/>
			  <br/>
			  <hc>$ montepython/MontePython.py info CHAINS --no-mean</hc>
			  <br/>
			  <br/>                        
The details for all the options of the 'info' command can be accessed via
  			  <br/>
			  <br/>
			  <hc>$ montepython/MontePython.py info --help</hc>
			  <br/>
			  <br/>
                          It is possible to read and reanalyze  Planck chains with Montepython.
			<br/>
			<br/>
			<b>Running in parallel</b>
			<br/>
			<br/>
			MCMC chains can be run in parallel using <a href="http://www.open-mpi.org/" target="_blank">Open MPI</a>, and in particular the command <hc>mpirun</hc>. In order to do this with Monte Python a few softwares have to be installed. 
			  <br/>
			  <br/>
			  For mpi4py, which is necessary in order to use the command <hc>mpirun</hc> with MontePython, you need to install openmpi first and then:
			  <br/>
			  <br/>
			  <hc>$ pip install mpi4py --user</hc>
			  <br/>
			  <br/>
			  Note that the <hc>--user</hc> option is to be used when you do not have the admin privileges, which is generally the case on clusters.
			  <br>
			    <br>
			      To install openmpi, download and unzip it from <a href="http://www.open-mpi.org/software/ompi/v1.10/" target="_blank">Open MPI</a> and then read the instruction in the <hc>INSTALL</hc> file, in the user section (basically <hc>./configure</hc> and <hc>sudo make install</hc>). Once these two things (mpi4py and MultiNest) are well set on you machine you should be able to run a few chains in parallel with
			  <br/>
			  <br/>
			  <hc>$ mpirun -np 10 montepython/MontePython.py run -p jla.param -o chains/JLAchains -N 1000</hc>
			  <br/>
			  <br/>
			  This will launch ten chains with one thousand steps each.
			  			<br/>
			<br/>If in the future you would like to use the Nested Sampling method, you should also install MultiNest and PyMultiNest, one after the other. For MultiNest do
			<br/>
			<br/>
			<hc>$ git clone http://github.com/JohannesBuchner/MultiNest.git<br/>
			  $ cd ./MultiNest/build/<br/>
			  $ cmake ..<br/>
			  $ make</hc>
			  <br/>
			  <br/>
			  Then add the following line into your <hc>.bash_profile</hc> file:
			  <br/>
			  <br/>
			  <hc>export LD_LIBRARY_PATH=/Path/to/MultiNest/lib:$LD_LIBRARY_PATH</hc>
			  <br/>
			  <br/>
			  Reload your bash profile with <hc>. .bash_profile</hc> (after you have <hc>cd</hc> to the directory where the file is). This will ensure that the libraries needed by Monte Python for parallel computing will be well linked. For PyMultiNest do:
						<br/>
			<br/>
			<hc>$ git clone http://github.com/JohannesBuchner/PyMultiNest<br/>
			  $ cd ./PyMultiNest<br/>
			  $ python setup.py install --user</hc>
			  <br/>
			  <br/>	
		   <div class="date">September 2015
	        </div>

		   <h4 id="CC-in2p3">Submitting MontePython jobs at CC-in2p3</h4>
<h1>
			We shall now describe how to submit jobs on a cluster, <i>i.e.</i>  running simultaneously a numbers of chains with several thousand points each.
		
		<br/>
		<br/>
The hardware of the in2p3 Computing Center is located in Lyon, Fr. It is a Sun Grid Engine cluster. All information about it can be found on the <a href="http://cc.in2p3.fr/docenligne/969#introduction" target="_blank">CC-in2p3 webpage</a>, and the <a href="http://www.ens-lyon.fr/PSMN/doku.php?id=documentation:examples:tutorials_scripts" target="_blank">ENS Lyon computing webpage</a>.
	<br/>
	<br/>
	After you have asked for a new account on your name you can login using the ssh command:
	  <br/>
	  <br/>
			  <hc>$ ssh username@ccage.in2p3.fr</hc>
	<br/>
	<br/>
	The little memory available in your home directory, <hc>/afs/in2p3.fr/home/username</hc>, will not allow you to perform long calculations and store the necessary data. As a member of the LSST or the HESS collaborations you will be able to use the SPS disk, from which you will launch your jobs. Create a new directory on this disk, for instance: <hc>/sps/lsst/data/username</hc>.
	<br/>
	<br/>
	In there, you should install Class, MontePython, the Planck Likelihood, the Planck data, PyMultiNest and MultiNest, following the indications given in the previous sections. Create a few subdirectories in order to keep everything tidy. In particular:
	<br/>
	<br/>
	1) <b>/PlanckMCMC</b><br><br>
	-/MultiNest<br>
	-/PyMultiNest<br>
	-/class_public-2.4.2<br>
	-/montepython_public-2.1.4<br>
	-/plc-2.0<br>
	-/plc_2.0<br><br>
	2) <b>/jobs</b><br><br>
	-/chains<br>
	-/output<br>
	<br/>
	<br/>
	3) <b>/scripts</b><br>
	<br/>
	<br/>
	Within this last directory create a new shell script, for instance <hc>JobMCMC.sh</hc>, with any text editor. This file contains two types of information:  the options for the computing and the commands to be executed in order to perform the calculation. Example:
		<br/>
		<br/>
		<div class="code">
#!/bin/sh<br/>
#$ -P P_lsst<br/>
#$ -q pa_long<br/>
#$ -l ct=100000<br/>
#$ -l sps=1<br/>
#$ -o /sps/lsst/data/username/jobs/output<br/>
#$ -cwd<br/>
#$ -j y<br/>
#$ -V<br/>
#$ -N MCMC_Run<br/>
#$ -pe openmpi 5<br/><br/>


cd ${SGE_O_WORKDIR}<br/><br/>

source /afs/in2p3.fr/home/b/bbolliet/.profile<br/><br/>

module load openmpi-x86_64<br/><br/>


EXECDIR="/sps/lsst/data/username/PlanckMCMC/montepython_public-2.1.4"<br/>
CHDIR="/sps/lsst/data/username/jobs/chains"<br/><br/>


mpirun -np ${NSLOTS} ${EXECDIR}/montepython/MontePython.py  run --conf ${EXECDIR}/default.conf -p ${EXECDIR}/base2015HighlNs.param -c ${EXECDIR}/covmat/base.covmat -b ${EXECDIR}/bestfit/base.bestfit -o ${CHDIR}/PlanckMPI -N 100
		</div>
<br/>
In order to use the <hc>pa_long</hc> queue you must ask the permission to the administrators of CC-in2p3 (without it you wont be able to use mpi properly).<br/><br/>
To launch a job do<br/><br/>
<hc>$ qsub scripts/JobMCMC.sh</hc>
<br><br>
    To check the status of your jobs do
    <br/><br/>
<hc>$ qstat</hc>
<br><br>
    To delete all your jobs do
        <br/><br/>
<hc>$ qdel -u username</hc>
<br><br>
        To delete a specific job do
        <br/><br/>
<hc>$ qdel JOB_ID</hc>
<br><br>
    where <hc>JOB_ID</hc> is a number that you can access with qstat.<br><br>
	Once you have enough chains so that the Gelman-Rubin convergence numbers are satisfying, analyze the chains with
	<br><br>
	    <hc>$ PlanckMCMC/montepython_public-2.1.4/montepython/MontePython.py info jobs/chains/PlanckMPI/2015*</hc>
	    <br><br>
		In particular, this will create a subdirectory with your plots which you can import on your desktop with
		<br><br>
		    <hc>$ scp -r username@ccage.in2p3.fr:/sps/lsst/data/username/jobs/chains/PlanckMPI/plots  /path/to/my/Desktop/Plots</hc>

		      <br><br>
                          from a local Terminal.<br><br>
                              IMPORTANT: at CC-Lyon, the chains are created too fast. At the start of a job montepython creates a log.param file, which is subsequently read each time a new chain is created. Generally, when the second chain starts, the log.param file is being read while it has not been written completely, then the job end with the error message "Information on the likelihood was not found in the log.param file".<br><br>

To overcome this issue copy your parameter file into your chain directory and rename it: <hc>log.param</hc>. 
                                  




	    	  <br/><br/>  
















			
<!--<h4>IPython<h1>
	    Nunc age iam deinceps cunctarum exordia rerum qualia sint et quam longe distantia formis percipe, multigenis quam sint variata figuris; non quo multa parum simili sint praedita forma, sed quia non volgo paria omnibus omnia constant. nec mirum; nam cum sit eorum copia tanta ut neque finis, uti docui, neque summa sit ulla, debent nimirum non omnibus omnia prorsum esse pari filo similique adfecta figura.-->

	<h4 id="ROOT">ROOT</h4>
<h1>
	    In order to analyze the data comming from colliders, particle physicists have been developping ROOT since 1995. It is a powerful software, able to compile C++ language on the go.<br/><br/>
	    The <a href="https://root.cern.ch/root/html534/guides/users-guide/ROOTUsersGuideA4.pdf" target="_blank">user guide</a> provides a presentation of the many aspects and utilities of ROOT.<br/><br/>
	    You can <a href="https://root.cern.ch/drupal/content/downloading-root" target="_blank">download</a> the latest version of ROOT, and follow the indications for the <a href="https://root.cern.ch/building-root" target="_blank">installation</a>.<br/><br/>
	    Alternatively, on a mac, you can install it with <hc>brew install root</hc>.<br/><br/>
	    The ROOT Graphical User Interface (GUI) is useful on its own. You can create, edit and save your plots in many different formats, in a user-friendly way.<br/><br/>
	    In order to access to the GUI, open ROOT, and open a TBrowser:<br/><br/>
	    <hc>TBrowser browser</hc>
	    <br/><br/>
	   Use the brower in order to locate tutorials in the <i>ROOT files</i>. The ROOT tutorials are full of examples and templates to start with. If you are using ROOT on a remote machine, you can still download them from <a href="https://root.cern.ch/root/html/tutorials/" target="_blank">there</a>.
<br>
<br>

In order to include ROOT libraries when you are compiling a code simply use, the '-I' option:<br>
<div class="code">
  $ g++ -I `root-config --incdir` -o MyExec main.C  `root-config --libs`
<br>
</div>
<br>
Be careful with the "`". 


	    		<br/>
		<br/>
		
		   <div class="date">August 2015
	        </div>

	    	    
		<h4 id="LyX">LyX</h4>
<h1>
		    <a href="http://www.lyx.org/" target="_blank">LyX</a> is a software initially designed to write LaTeX documents in a more user friendly way than the usual LaTeX editors. The first public version was released in 1995, when LaTeX was starting to be widely used in the scientific cummunity.
		<br/>
		<br/>
LyX can be used as a notebook. It is a perfect tool to write equations and do calculations. The codes for fractions, powers, etc. are just the same as in LaTeX, however the equations appears simultaneously on the lyx file. Then, it is possible to copy-paste whole parts of equations very easily, avoiding many mistakes we usually do when copying equations by hand.  
		<br/>
		<br/>
		You will find my LyX template <a href="Attachements/LyXTemplate.zip" download>here</a>.  Once you have installed LyX, open the lyx file. This template features: a BibTeX bibliography; a table of content; a table; a block of equation; an image; cross-references; header and footer.
		<br/>
		<br/>
	Click on the eyes logo on the left-hand corner and you will see the pdf version of the lyx file. Go to File/export/PDF (pdflatex) and export you document as a pdf. Do File/export/LaTeX (standard) for the tex version.
		<br/>
		<br/>
		Most of the important LaTeX settings you want to use can be specified in Document/Settings and then Document Class, Latex Preambule, etc.
		<br/>
		<br/>
		
		   <div class="date">August 2015</div>

	<h4 id="Gnuplot">Gnuplot</h4>
<h1>
		   Gnuplot is a plotting utility relatively simple to use. On  <a href="http://www.gnuplot.info/" target="_blank">Gnuplot</a> homepage you will be able to download the software and related documentation.
		<br/>
		<br/>
A set of illustrative example of commands can be found <a href="http://alvinalexander.com/technology/gnuplot-charts-graphs-examples" target="_blank">there</a>. 
		<br/>
		<br/>
For instance, say you have a data file called 'data.txt', with two columns. Move to the repository where this file is located with the terminal. Then, do:
		<br/>
		<br/>
<div class="code">
$ gnuplot <br>
gnuplot> plot 'data.txt' using 1:2 w l title 'TEST'<br>
gnuplot> set key top left <br>
gnuplot> set logscale x<br>
gnuplot> set logscale y<br>
gnuplot> set xlabel 'multipole l'<br>
gnuplot> set ylabel 'l(l+1)Cl/2Pi [uK^2]'<br>
gnuplot> set terminal png<br>
gnuplot> set output "plot.png"<br>
gnuplot> replot<br>
gnuplot> exit
</div>	
<br>
                                 <br>
This will creat the file 'plot.png' in the current directory. 	
		<br/>
		<br/>
		   <div class="date">October 2015</div>


	<h4 id="GitHub">GitHub</h4>
<h1>
<a href="https://github.com/" target="_blank">GitHub</a> is a hosting platform dedicated to the management of collaborative projects involving computer codes and softwares. 
<br>
<br>
Once you have created your free personal account, follow the instructions bellow for a quick start.
<br>
<br>
Login to your account. Create a new repository, clicking the green button to the left: <i>'+ New repository'</i>.
<br>
<br>
Give a one sentence description of what you are goint to store in the Description field.
<br>
<br>
Check the box <i>'Public'</i> and then check '<i>Initialize this repository with a readme</i>'. Click the button 'Create repository' finally.
<br>
<br>
The repository MyNewRep/ is created. It contains a single 'README.md' file with the description you just gave.
<br>
<br>
Open a terminal, check that <hc>git</hc> is installed as a command line, by typing for instance git --version. Install it if it is not. 
<br>
<br>
Then, with the terminal move to the directory which you want to <i>push</i> (copy) on GitHub, and do  
<br>
<br>
<div class="code">
$ git clone https://github.com/username/MyNewRep<br>
$ cd ./MyNewRep
</div>	
<br>
Copy paste all your project files into MyNewRep/. And then do
<br>
<br>
<div class="code">
$ git add .<br>
$ git commit -m "FirstUpdate" <br>
$ git push origin master
</div>
<br>
You can check online that all the files have appeared in the online directory.
<br>
<br>
Back to your GitHub account, click on your folder MyNewRep/. Create a new <i>branch</i> by clicking on the button 'Branch:master' and entering the name of your first branch, for instance <i>MyFirstBranch</i>. This is nothing else than a secondary copy of your folder.    
<br>
<br>
The philosophy is to work on secondary branches and when a secondary branch is satisfying one eventually merges with the master branch.    
<br>
<br>
Back to the local repository MyNewRep/, that you access with the terminal, you shall create a new branch called <i>MyFirstBranch</i> too. Procede as follow,
<br>
<br>
<div class="code">
$ git checkout -b MyFirstBranch
</div>
<br>
Then
<br>
<br>
<div class="code">
$ git branch -a 
</div>
<br>
to list all your branches. <br><br>
You shall now perform changes and do <br><br>
<div class="code">
$ git add .<br>
$ git commit -m "Update on branch"<br>
$ git push origin MyFirstBranch
</div>
<br>
Check online that your changes have only appeared on the MyFirstBranch copy. Click on the button 'compare' to the left in order to see the differences between branches.

<br>
<br>
		   <div class="date">October 2015</div>


<h4 id="CreateYourWebsite">Create your own website</h4>
  <h1>
	  You can easily create your personal webpage for free using an HTML/CSS template and <a href="https://github.com/" target="_blank">GitHub</a>.
	  <br/>
	  <br/>
	  Browse the internet and find a template that you like. Usually you will be able to download a folder that contains the files "index.html" (for the content of your webpage) and "style.css" (for the layout). You are welcome to use <a href="Attachements/HTML-CSS.zip" download>my</a> HTML/CSS files to start with. The html files can be opened with your internet browser. In order to edit the html and css files you can use  any text editor. 
  <br />
  <br />
  Once you are happy with your website, store it all on your desktop in a repository called WEBSITE. The next step is to bring it online using GitHub. 
<br />
<br />
Register onto GitHub. The registration is free as long as you are using public repositories only. Let us assume your username is richardfeynman, then you have to create a public repository called "richardfeynman.github.io".
<br />
<br />
Now you shall import the content of the folder WEBSITE/ into richardfeynman.github.io/ by following the next instructions.
<br />
<br />
Open a terminal, check that <hc>git</hc> is installed as a command line, by typing for instance <hc>git --version</hc>. Install it if it is not. Then type:
<br />
<br />
<hc>git clone https://github.com/richardfeynman/richardfeynman.github.io</hc>
<br />
<br />
Move to the newly created richardfeynman/ folder with 
<br />
<br />
<hc>cd ./richardfeynman</hc>
<br />
<br />
Copy-paste the whole content of the WEBSITE/ folder into the richardfeynman/ folder. And type the following commands, one by one, in your terminal:
<br />
<br />
<hc>git add .</hc>
<br />
<hc>git commit -m "Putting my website online"</hc>
<br />
<hc>git push origin master</hc>
<br />
<br />
Thanks to the first two commands <hc>git</hc> loads the files you have just copy-pasted. The message "Putting my website online" could be anything, it will appear next to the files as a description when you look into richardfeynman.github.io/ on your GitHub account.
<br />
<br />
The third command is to tell <hc>git</hc> to <i>push</i> the content of your local richardfeynman/ folder (identified as <i>origin</i>, the local <hc>git</hc> repository) onto your online repository richardfeynman.github.io/ (identified as <i>master</i>, the remote (online) <hc>git</hc> repository). 
<br />
<br />
After the last command you need to provide <hc>git</hc> with your username and password. 
<br />
<br />
You can see what the online repository richardfeynman.github.io/ should look like by looking at my <a href="https://github.com/borisbolliet/borisbolliet.github.io" target="_blank">borisbolliet.github.io/</a> (by the way, note that anyone can see and download the content of your public repository on GitHub).
<br />
<br />
Your website is now online at: http://richardfeynman.github.io and will soon be listed in the popular search engines.  
<br />
<br />
At this stage you do not need your WEBSITE/ folder anymore, you can delete it and rename your local <hc>git</hc> repository richardfeynman/ to WEBSITE/ or whatever.
<br />
<br />
Whenever you wish to update your website, just add, delete and update all you need in the local <hc>git</hc> repository. You can modify your existing files, delete some of them, add new ones, etc. Then, open a terminal and <hc>cd</hc> to the local <hc>git</hc> repository  and repeat the commands:
<br />
<br />
<hc>git add .</hc>
<br />
<hc>git commit -m "My first update"</hc>
<br />
<hc>git push origin master</hc>
<br />
<br />
</h1>

<hst id="WebsiteName">Change your website name extension</hst><br><br>
<h1>
It is possible to change your website name extension (<i>.github.io</i>) so that your website would be called <a href="http://www.richardfeynman.com/" target="_blank">richardfeynman.com</a> instead of richardfeynman.github.io, however you would need to pay about seven euros per year.
<br />
<br />
In order do this, go to a domain name provider and purchase the domain name of your choice (if still available). The most popular domain name providers seem to be this <a href="https://uk.godaddy.com/?countryview=1" target="_blank">one</a> and this <a href="https://www.name.com/" target="_blank">one</a>. 
<br />
<br />
Say you have purchased the domain name <a href="http://www.richardfeynman.com/" target="_blank">www.richardfeynman.com</a>. The only thing you have to do concerning GitHub is to create a file called "CNAME" and place it into your  richardfeynman.github.io/ online repository (with the procedure described above). This file has to contain the single following line: 
<br />
<br />
<hc>richardfeynman.com</hc>
<br />
<br />
Note the absence of the "www". (You can download the CNAME file <a href="./Attachements/CNAME" download>here</a> and adapt it to whatever your domain name is.) Do not forget to update your online richardfeynman.github.io/ repository and you are done with GitHub.
<br />
<br />
The final steps is to sign in onto your domain name provider account and, for your new domain name, set the IP adress of the host (GitHub) to <hc>192.30.252.153</hc>, as well as the directory onto which your domain name has to point to, that is richardfeynman.github.io, under the "www" section of the CName alias. You will find a very clear up-to-date description of these steps on <a href="https://medium.com/@LovettLovett/github-pages-godaddy-f0318c2f25a" target="_blank">Julia Lovett</a>'s website.
		
		<br/>
		<br/>
		   <div class="date">August 2015</div>

      </div>
    </div>

  </div>
    </div>


    
</body>
</html>
